{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chef Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/gargoyle/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import wordnet  # for synonyms\n",
    "\n",
    "nltk.download('wordnet')\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"False\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training samples: 2399\n",
      "\n",
      "Current class distribution:\n",
      "chef_id\n",
      "1533    323\n",
      "3288    361\n",
      "4470    645\n",
      "5060    427\n",
      "6357    298\n",
      "8688    345\n",
      "Name: count, dtype: int64\n",
      "→ Target per class: 645\n",
      "\n",
      "↑ Augmented class 5060: +218 samples\n",
      "↑ Augmented class 3288: +284 samples\n",
      "↑ Augmented class 8688: +300 samples\n",
      "↑ Augmented class 1533: +322 samples\n",
      "↑ Augmented class 6357: +347 samples\n",
      "\n",
      "✅ Added total augmented samples: 1471\n",
      "✅ New total dataset size: 3870\n",
      "=====================================\n",
      "\n",
      "Training samples after augmentation: 3870\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/gargoyle/Downloads/FINAL TESTING/train.csv', sep=';')\n",
    "df['tags'] = df['tags'].apply(ast.literal_eval)\n",
    "df['description'] = df['description'].fillna('')\n",
    "\n",
    "# aug functions\n",
    "def get_synonym(word):\n",
    "    \"\"\"Return a random synonym using WordNet.\"\"\"\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            candidate = lemma.name().replace('_', ' ')\n",
    "            if candidate.lower() != word.lower() and len(candidate.split()) == 1:\n",
    "                synonyms.add(candidate)\n",
    "    if synonyms:\n",
    "        return np.random.choice(list(synonyms))\n",
    "    return word\n",
    "\n",
    "\n",
    "def synonym_replace(text, replacement_prob=0.15):\n",
    "    words = text.split()\n",
    "    if len(words) < 5:\n",
    "        return text\n",
    "    new_words = [\n",
    "        get_synonym(w) if np.random.rand() < replacement_prob else w\n",
    "        for w in words\n",
    "    ]\n",
    "    return ' '.join(new_words)\n",
    "\n",
    "\n",
    "def random_delete(text, deletion_prob=0.05):\n",
    "    words = text.split()\n",
    "    if len(words) < 5:\n",
    "        return text\n",
    "    kept = [w for w in words if np.random.rand() > deletion_prob]\n",
    "    return ' '.join(kept) if len(kept) > 3 else text\n",
    "\n",
    "\n",
    "def sentence_shuffle(text):\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    if len(sentences) < 3:\n",
    "        return text\n",
    "    np.random.shuffle(sentences)\n",
    "    return ' '.join(sentences)\n",
    "\n",
    "\n",
    "def augment_text(text):\n",
    "    r = np.random.rand()\n",
    "    if r < 0.5:\n",
    "        return synonym_replace(text)\n",
    "    elif r < 0.75:\n",
    "        return random_delete(text)\n",
    "    else:\n",
    "        return sentence_shuffle(text)\n",
    "\n",
    "\n",
    "def augment_tags(tag_list):\n",
    "    \"\"\"Slightly modify tags for diversity (swap or drop).\"\"\"\n",
    "    if not isinstance(tag_list, list) or len(tag_list) == 0:\n",
    "        return tag_list\n",
    "    new_tags = tag_list.copy()\n",
    "    # 10% chance to remove one tag\n",
    "    if np.random.rand() < 0.1 and len(new_tags) > 1:\n",
    "        del new_tags[np.random.randint(0, len(new_tags))]\n",
    "    # 10% chance to duplicate another\n",
    "    if np.random.rand() < 0.1:\n",
    "        new_tags.append(np.random.choice(new_tags))\n",
    "    return list(set(new_tags))\n",
    "\n",
    "\n",
    "# balancing chef data\n",
    "def balance_and_augment(df, label_col='chef_id', id_col='orig_id'):\n",
    "    \"\"\"Upsample minority chefs with text+tag augmentation, tracking lineage.\"\"\"\n",
    "    assert id_col in df.columns, f\"{id_col} must be present before augmenting.\"\n",
    "\n",
    "    label_counts = df[label_col].value_counts()\n",
    "    max_count = label_counts.max()\n",
    "    augmented_rows = []\n",
    "\n",
    "    print(\"\\nCurrent class distribution:\")\n",
    "    print(label_counts.sort_index())\n",
    "    print(f\"→ Target per class: {max_count}\\n\")\n",
    "\n",
    "    for chef, count in label_counts.items():\n",
    "        if count < max_count:\n",
    "            n_to_add = max_count - count\n",
    "            chef_df = df[df[label_col] == chef]\n",
    "            sampled = chef_df.sample(n=n_to_add, replace=True, random_state=42).copy()\n",
    "\n",
    "            # lineage + flags\n",
    "            sampled['parent_id'] = sampled[id_col]\n",
    "            sampled['is_augmented'] = True\n",
    "\n",
    "            # apply augments\n",
    "            sampled['description'] = sampled['description'].apply(augment_text)\n",
    "            sampled['tags'] = sampled['tags'].apply(augment_tags)\n",
    "\n",
    "            augmented_rows.append(sampled)\n",
    "            print(f\"↑ Augmented class {chef}: +{n_to_add} samples\")\n",
    "\n",
    "    base = df.copy()\n",
    "    base['parent_id'] = base[id_col]\n",
    "    base['is_augmented'] = False\n",
    "\n",
    "    if augmented_rows:\n",
    "        df_aug = pd.concat([base] + augmented_rows, ignore_index=True)\n",
    "        print(f\"\\nAdded total augmented samples: {sum(len(r) for r in augmented_rows)}\")\n",
    "        print(f\"New total dataset size: {len(df_aug)}\")\n",
    "        print(\"=====================================\\n\")\n",
    "        return df_aug\n",
    "    else:\n",
    "        print(\"No augmentation performed — already balanced.\\n\")\n",
    "        return base\n",
    "    \n",
    "# Split before aug (keep validation clean)\n",
    "X_raw = df[['description', 'tags', 'chef_id']].copy()\n",
    "\n",
    "X_train_raw, X_val_raw = train_test_split(\n",
    "    X_raw, test_size=0.2, random_state=4, stratify=X_raw['chef_id']\n",
    ")\n",
    "\n",
    "assert not set(X_train_raw.index).intersection(set(X_val_raw.index)), \"Leakage detected between train and val!\"\n",
    "\n",
    "\n",
    "# Reset indices\n",
    "X_train_raw = X_train_raw.reset_index(drop=True)\n",
    "X_val_raw = X_val_raw.reset_index(drop=True)\n",
    "X_train_raw['orig_id'] = X_train_raw.index\n",
    "\n",
    "print(f\"Original training samples: {len(X_train_raw)}\")\n",
    "\n",
    "# aug only training set\n",
    "X_train_aug = balance_and_augment(X_train_raw)\n",
    "print(f\"Training samples after augmentation: {len(X_train_aug)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Embeddings for Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 121/121 [00:39<00:00,  3.06it/s]\n",
      "Batches: 100%|██████████| 19/19 [00:06<00:00,  2.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final training shape: (3870, 1147)\n",
      "Validation shape: (600, 1147)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/Users/gargoyle/Library/Python/3.9/lib/python/site-packages/sklearn/preprocessing/_label.py:909: UserWarning: unknown class(es) ['burgers', 'catfish', 'collard-greens', 'filipino', 'goose', 'mussels', 'pasta-shells', 'pennsylvania-dutch', 'sourdough', 'turkey-burgers'] will be ignored\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embedding_model = SentenceTransformer('all-mpnet-base-v2')  # 768 dim\n",
    "\n",
    "# Encode augmented train and untouched val descriptions\n",
    "desc_train_emb = embedding_model.encode(X_train_aug['description'].tolist(), show_progress_bar=True)\n",
    "desc_val_emb   = embedding_model.encode(X_val_raw['description'].tolist(), show_progress_bar=True)\n",
    "\n",
    "desc_train_emb_df = pd.DataFrame(desc_train_emb, columns=[f'desc_emb_{i}' for i in range(768)], index=X_train_aug.index)\n",
    "desc_val_emb_df   = pd.DataFrame(desc_val_emb,   columns=[f'desc_emb_{i}' for i in range(768)], index=X_val_raw.index)\n",
    "\n",
    "# One-hot encode tags\n",
    "mlb = MultiLabelBinarizer()\n",
    "tags_train_encoded = mlb.fit_transform(X_train_aug['tags'])\n",
    "tags_val_encoded   = mlb.transform(X_val_raw['tags'])\n",
    "\n",
    "tags_train_df = pd.DataFrame(tags_train_encoded, columns=mlb.classes_, index=X_train_aug.index).add_prefix('tag_')\n",
    "tags_val_df   = pd.DataFrame(tags_val_encoded,   columns=mlb.classes_, index=X_val_raw.index).add_prefix('tag_')\n",
    "\n",
    "# Combine embeddings + tag features\n",
    "X_train = pd.concat([desc_train_emb_df, tags_train_df], axis=1)\n",
    "X_val   = pd.concat([desc_val_emb_df, tags_val_df], axis=1)\n",
    "\n",
    "y_train = X_train_aug['chef_id'].reset_index(drop=True)\n",
    "y_val   = X_val_raw['chef_id'].reset_index(drop=True)\n",
    "\n",
    "train_meta = X_train_aug[['is_augmented', 'parent_id']].copy()\n",
    "train_meta.index = X_train.index  # ensure identical index alignment\n",
    "\n",
    "print(f\"\\nFinal training shape: {X_train.shape}\")\n",
    "print(f\"Validation shape: {X_val.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing 7 C values with lbfgs solver...\n",
      "  C= 0.001 | Train: 0.9444 | Val: 0.8367 | Gap: 0.1078\n",
      "  C= 0.005 | Train: 0.9822 | Val: 0.8517 | Gap: 0.1305\n",
      "  C= 0.010 | Train: 0.9930 | Val: 0.8500 | Gap: 0.1430\n",
      "  C= 0.050 | Train: 1.0000 | Val: 0.8417 | Gap: 0.1583\n",
      "  C= 0.100 | Train: 1.0000 | Val: 0.8417 | Gap: 0.1583\n",
      "  C= 0.500 | Train: 1.0000 | Val: 0.8383 | Gap: 0.1617\n",
      "  C= 1.000 | Train: 1.0000 | Val: 0.8367 | Gap: 0.1633\n",
      "\n",
      "============================================================\n",
      "Best C: 0.005\n",
      "Best validation accuracy: 0.8517\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "scaler_hp = StandardScaler()\n",
    "X_train_scaled = scaler_hp.fit_transform(X_train)\n",
    "X_val_scaled = scaler_hp.transform(X_val)\n",
    "\n",
    "C_values = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0]\n",
    "results = []\n",
    "\n",
    "print(f\"\\nTesting {len(C_values)} C values with lbfgs solver...\")\n",
    "for C in C_values:\n",
    "    model = LogisticRegression(C=C, max_iter=1000, random_state=42, n_jobs=-1, solver='lbfgs')\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    train_score = model.score(X_train_scaled, y_train)\n",
    "    val_score = model.score(X_val_scaled, y_val)\n",
    "    results.append({'C': C, 'train_acc': train_score, 'val_acc': val_score, 'gap': train_score - val_score})\n",
    "    print(f\"  C={C:6.3f} | Train: {train_score:.4f} | Val: {val_score:.4f} | Gap: {train_score - val_score:.4f}\")\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "best_idx = results_df['val_acc'].idxmax()\n",
    "best_C = results_df.loc[best_idx, 'C']\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Best C: {best_C}\")\n",
    "print(f\"Best validation accuracy: {results_df.loc[best_idx, 'val_acc']:.4f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Leakage-proof CV (val=originals only, train=originals(excl. fold) + augmented not derived from val):\n",
      "  Fold 1: 0.8567 (85.67%)\n",
      "  Fold 2: 0.8533 (85.33%)\n",
      "  Fold 3: 0.8433 (84.33%)\n",
      "  Fold 4: 0.8567 (85.67%)\n",
      "  Fold 5: 0.7933 (79.33%)\n",
      "  Fold 6: 0.8500 (85.00%)\n",
      "  Fold 7: 0.8400 (84.00%)\n",
      "  Fold 8: 0.8328 (83.28%)\n",
      "--------------------\n",
      "Mean: 0.8408 (84.08%)\n",
      "Std:  0.0196 (1.96%)\n",
      "Min:  0.7933 (79.33%)\n",
      "Max:  0.8567 (85.67%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Originals only for validation candidates\n",
    "orig_mask = ~train_meta['is_augmented']\n",
    "orig_idx = X_train.index[orig_mask]\n",
    "\n",
    "X_orig = X_train.loc[orig_idx]\n",
    "y_orig = y_train.loc[orig_idx]\n",
    "\n",
    "skf = StratifiedKFold(n_splits=8, shuffle=True, random_state=42)\n",
    "fold_scores = []\n",
    "\n",
    "print(\"\\nLeakage-proof CV (val=originals only, train=originals(excl. fold) + augmented not derived from val):\")\n",
    "for fold, (tr_i, va_i) in enumerate(skf.split(X_orig, y_orig), 1):\n",
    "    val_idx = X_orig.index[va_i]          # original rows for validation\n",
    "    tr_orig_idx = X_orig.index[tr_i]      # original rows for training\n",
    "\n",
    "    # aug rows whose parent is not in validation fold\n",
    "    aug_ok_mask = (train_meta['is_augmented']) & (~train_meta['parent_id'].isin(val_idx))\n",
    "    tr_aug_idx = X_train.index[aug_ok_mask]\n",
    "\n",
    "    # final train = originals (train part) + safe aug\n",
    "    tr_idx_all = tr_orig_idx.union(tr_aug_idx)\n",
    "\n",
    "    X_tr = X_train.loc[tr_idx_all]\n",
    "    y_tr = y_train.loc[tr_idx_all]\n",
    "    X_va = X_train.loc[val_idx]\n",
    "    y_va = y_train.loc[val_idx]\n",
    "\n",
    "    # scale from training only\n",
    "    scaler_cv = StandardScaler()\n",
    "    X_tr_s = scaler_cv.fit_transform(X_tr)\n",
    "    X_va_s = scaler_cv.transform(X_va)\n",
    "\n",
    "    model_cv = LogisticRegression(C=best_C, max_iter=1000, random_state=42, n_jobs=-1, solver='lbfgs')\n",
    "    model_cv.fit(X_tr_s, y_tr)\n",
    "    preds = model_cv.predict(X_va_s)\n",
    "    acc = accuracy_score(y_va, preds)\n",
    "    fold_scores.append(acc)\n",
    "    print(f\"  Fold {fold}: {acc:.4f} ({acc:.2%})\")\n",
    "\n",
    "print(\"-\" * 20)\n",
    "print(f\"Mean: {np.mean(fold_scores):.4f} ({np.mean(fold_scores):.2%})\")\n",
    "print(f\"Std:  {np.std(fold_scores):.4f} ({np.std(fold_scores):.2%})\")\n",
    "print(f\"Min:  {np.min(fold_scores):.4f} ({np.min(fold_scores):.2%})\")\n",
    "print(f\"Max:  {np.max(fold_scores):.4f} ({np.max(fold_scores):.2%})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
